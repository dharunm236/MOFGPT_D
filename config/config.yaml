random_seed: 42
project_name: "mof_gpt_pretrain"
eval_mode: false
resume_training: true

data:
  train_csv_filename: "../benchmark_datasets/gpt_train.csv"
  test_csv_filename: "../benchmark_datasets/gpt_test.csv"
  vocab_path: "../tokenizer/vocab_with_eos.txt"
  topology_labels_map_filename: "../tokenizer/topology_labels_map.json"
  train_test_ratio: 0.8
  batch_size: 128 #4
  num_workers: 8 #1
  ignore_index: -100
  use_multiprocessing: False
  tokenizer:
    max_seq_len: 512
    pad_token: '[PAD]'
    mask_token: '[MASK]'
    bos_token: '[BOS]'
    eos_token: '[EOS]'
    unk_token: '[UNK]'
    add_special_tokens: True
    truncation: True
    use_topology: True

training:
  device: "cuda"
  epochs: 30
  fp16: True
  top_ks: [1, 3, 5]
  batch_size: 32 #4
  optimizer:
    lr: 0.001
    type: "AdamW"
    weight_decay: 0.001
    gradient_accumulation_steps: 1
  scheduler:
    type: "cosine"
    warmup_ratio: 0.03
  logging_ratio: 0.05
  save_ratio: 0.5
  save_dir: "../saved_models/"

model:
  model_config_filename: "../config/model.yaml"
