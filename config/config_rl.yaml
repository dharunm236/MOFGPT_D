training:
  saved_state_dict_filename: "saved_models/llama2_best2.pt"
  saved_state_dict_url: "https://drive.google.com/file/d/1uuOjMQJc5seEPkak7gRse4N5HSssmhVp/view?usp=sharing"
  num_samples_per_epoch: 2048
  epochs: 10
  fp16: True
  optimizer:
    lr: 0.0001
    type: "AdamW"
    weight_decay: 0.001
  logging_ratio: 0.05
  save_ratio: 0.5
  save_dir: "saved_models/"
  model_name: "llama2_rl"
  batch_size: 128
eval:
  num_samples: 128
  eval_interval: 1
  print_smiles: True
model:
  model_name: "llama2_rl"
  use_cache: False
  return_dict: True
  output_attentions: False
  output_hidden_states: False
sampling:
  max_seq_len: 512
  batch_size: 32
  do_sample: True
  top_k: 1000
  top_p: 0.95
  temperature: 0.7
  constant_temperature: False
  temperature_std: 1.0
  early_stopping: False
  num_beams: 1
  num_beam_groups: 1
reward:
  sep_token: "&&"
  name: "basic_rules"
  discount_factor: 0.99
  basic_rules:
    single_sep_reward: 1.5
    multiple_sep_reward: -1
    no_sep_reward: -1.5
    eos_reward: 0.5
    no_eos_reward: -0.5
